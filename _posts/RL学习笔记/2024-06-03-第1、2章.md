# 第1章 强化学习基础

这里是《动手学强化学习》的学习笔记。

## 强化学习

### 定义

强化学习是通过智能体与环境交互来实现目标的算法。

#### 智能体（Agent）

智能体有三种关键要素：感知、决策共和奖励。

* 智能体可以感知 **环境**的**状态**。
* 智能体根据当前的状态做出**决策**。
* 环境根据智能体做出的决策，反馈给智能体一个**奖励**。

#### 环境

环境是动态的，它会随着某些因素（可能与智能体相关，也可能无关，但是强化学习的建模过程只关心智能体与环境交互所带来的变化）的变化而不断演进，因此使用随机过程来刻画。与智能体无关的变量使用$\cdot$表示。

$$
下一刻状态 \sim P(\cdot|当前状态，智能体动作)
$$

#### 目标

于是智能体与环境的每一次交互都会得到一个奖励，整个交互过程的所有奖励信号累加得到**回报（return）**。强化学习的建模方法（随机过程）关注回报的期望。智能体在学习过程中的优化目标正式回报期望最大化。

### 与监督学习的区别

#### 监督学习

训练数据采样自一个给定的数据分布，它的学习过程就是不断拟合这个数据分布。这个数据分布是静态的、不变的（独立同分布假设）。

$$
最优模型=argmin_{模型}E_{(特征，标签)\sim数据分布}[损失函数(标签，模型(特征))])
$$

监督学习寻找模型（对于深度学习而言就是给定模型结构的条件下学习模型参数），修改目标函数而数据分布不变。注意，这里的目标函数指$损失函数(模型())$。

#### 强化学习

训练数据从环境交互中得到的，因此智能体策略不同，与环境交互所产生的数据分布就不同。

$$
最优模型=argmin_{策略}E_{(状态，动作)\sim策略的占用度量}[奖励函数(状态，动作)])
$$

强化学习寻找策略，修改数据分布而目标函数不变。

# 第2章 多臂老虎机问题

## 问题定义

有一个拥有$K$根拉杆的老虎机，每根拉杆对应一个奖励概率分布$R$。每次拉动一根拉杆，从对应的$R$中获得奖励$r$。在$R$未知的条件下，如何拉动拉杆$T$次获得最高累计奖励。

trade-off：【探索拉杆获奖概率】 or 【只选择探索过的获奖概率概率最高的拉杆】

## 数学描述

多臂老虎机问题可以表示为一个元组$<\mathcal{A},\mathcal{R}>$。

* $\mathcal{A}$是动作集合，$a\in \mathcal{A}$。$a$表示从K根杠杆中拉动某一根拉杆。
* 每根拉杆对应一个奖励概率分布$\mathcal{R}`(r|a)$。

每个时间步只能拉动一个杠杆，总共有$T$个时间步。目标为$max \sum^{T}_{t=1}r_t$，其中$r_t\sim \mathcal{R}(\cdot|a_t))$。

## 解决方法

### 目标函数-累积懊悔

每一个动作$a$的期望是$Q(a)=E_{r \sim \mathcal{R}(\cdot|a)}[r]$。于是至少存在一根拉杆，其期望奖励不小于拉动其他任意一根拉杆，最优期望表示为$Q^*=max_{a \in A}Q(a)$。**懊悔（regret）**定义为当前动作的期望与最优期望的差值，$R(a)=Q^*-Q(a)$。累积懊悔定义为操作$T$次拉杆后懊悔的总量$\sigma_R=\Sigma^T_{t=1}R(a_t)$。于是最大化累积奖励就转化为最小化累积懊悔。

### 估计期望奖励

多次拉动同一根拉杆，计算得到奖励的平均值，从而估计拉动这根拉杆的期望奖励。

$$
Q_k=\frac{1}{k}\Sigma^k_{i=1}r_i=Q_{k-1}+\frac{1}{k}[r_k-Q_{k-1}]
$$

### Trade-off

【探索拉杆获奖概率】 or 【只选择探索过的获奖概率概率最高的拉杆】

#### $\epsilon$-贪婪算法

* 每次以概率为$1-\epsilon$选择以往经验中期望奖励估计最大的拉杆
* 每次以概率为$\epsilon$选择随机拉一个拉杆
  $$
  a_t= \begin{cases} argmax_{a \in \mathcal{A}} \hat{Q}(a),& 采样概率:1-\epsilon \\ randomChoice(\mathcal{A}), & 采样概率:\epsilon \end{cases}
  $$

实验结果表明，随着时间做反比例衰减的$\epsilon-贪婪算法$能够使得累积懊悔随时间步次线性地增长。

#### 上至信界算法

上置信界法引入了不确定度量$U(a)$，并使用了霍夫丁不等式：

$$
P(E[X]\geq \bar{x_t}+u) \leq e^{-2nu^2}
$$

$X_1,...,X_n$为$n$个独立同分布的随机变量，取值范围$[0,1]$，其经验期望为$\bar{x_n}= \frac{1}{n} \Sigma^n_{j=1} X_j$。
令$\hat{x_t}=\hat{Q}(a_t)$，$u=\hat{U}(a_t)$，$p=\frac{1}{t}$, 得到$a_t=argmax_{a \in \mathcal{A}}[\hat{Q}(a)+ \hat{U}(a)]$，其中$\hat{U}(a_t)= \sqrt{\frac{log(t)}{2(N(a_t)+1)}}$

#### 汤普森采样法

汤普森采样算法是一种计算所有拉杆最高奖励的蒙特卡洛采样方法：若某根拉杆被拉动$k$次，其中$m_1$次奖励为1，$m_2$次奖励为0，则该拉杆的奖励服从参数为$(m_1+1, m_2+1)$的Beta分布。

## 小结

多臂老虎机问题与强化学习的区别在于智能体与环境的交互不会改变环境，即没有**状态（state）**变化。
